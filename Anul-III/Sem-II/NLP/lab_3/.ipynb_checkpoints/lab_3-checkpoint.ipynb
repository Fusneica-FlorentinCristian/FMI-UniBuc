{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc86a84",
   "metadata": {
    "id": "4fc86a84"
   },
   "source": [
    "## Lab 3. Text classification using Bag-of-Words representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58ea606b",
   "metadata": {
    "id": "58ea606b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db25202",
   "metadata": {
    "id": "8db25202"
   },
   "source": [
    "### 1. Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee75112f",
   "metadata": {
    "id": "ee75112f"
   },
   "source": [
    "Am vazut in laboratorul trecut cum putem folosi diverse tehnici pentru a reduce un text la o lista de cuvinte (tokens). Am vazut cum putem elimina cuvintele care au o incarcatura semantica redusa (stopwords) si cum putem normaliza tokenii, aducandu-i la o forma unica pentru inlfexiuni diferite ale acestora (lematizare, stemming).\n",
    "\n",
    "Pentru a antrena un model de Machine Learning avem nevoie sa mai facem cativa pasi. Aceste reprezentari determinate anterior (liste de tokens) trebuie traduse in reprezentari numerice vectoriale (fiecarui text ii vom asocia un vector, iar toti vectorii corespunzatori textelor vor avea aceeasi dimensiune)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3168a61",
   "metadata": {
    "id": "a3168a61"
   },
   "source": [
    "O astfel de metoda este reprezentarea textelor folosind tehnica Bag-of-Words. Aceasta metoda porneste de la un vocabular de cuvinte, un sir $[w_1, w_2, \\ldots, w_n]$, unde $w_i$ reprezinta un cuvant din vocabular. Vocabularul poate fi stabilit de la inceput sau determinat pe baza listelor preprocesate de tokeni (putem defini vocabularul ca cei mai frecventi $n$ tokens din aceste liste).\n",
    "\n",
    "Un text (lista de tokens) poate fi acum transformat intr-un vector de dimensiune $n$: $(a_1, \\ldots, a_n)$ unde $a_i=1$ daca cuvantul $w_i$ apare cel putin o data in text, respectiv $a_i=0$ altfel. Aceasta reprezentare o mai numim si Bag-of-Words binar.\n",
    "\n",
    "O altfel de reprezentare asemanatoare poate fi bazata pe frecvente, $a_i$ reprezinta numarul de aparitii ale cuvantului $w_i$ in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae68493f",
   "metadata": {
    "id": "ae68493f"
   },
   "outputs": [],
   "source": [
    "# exemplu texte (procesate ca liste de tokens)\n",
    "texts = [\n",
    "    [\"love\", \"fun\", \"fun\", \"play\", \"happy\", \"sad\"],\n",
    "    [\"love\", \"happy\", \"love\", \"love\", \"tears\"],\n",
    "    [\"sad\", \"tears\", \"tears\"],\n",
    "]\n",
    "\n",
    "# exemplu vocabular\n",
    "vocab = [\"love\", \"fun\", \"tears\", \"sad\", \"happy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26baa2df",
   "metadata": {
    "id": "26baa2df"
   },
   "outputs": [],
   "source": [
    "# reprezentare BoW binar\n",
    "repr_bin = [\n",
    "    [1, 1, 0, 1, 1],\n",
    "    [1, 0, 1, 0, 1],\n",
    "    [0, 0, 1, 1, 0],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49ed4c0d",
   "metadata": {
    "id": "49ed4c0d"
   },
   "outputs": [],
   "source": [
    "# reprezentare BoW pe baza de frecvente\n",
    "repr_fr = [\n",
    "    [1, 2, 0, 1, 1],\n",
    "    [3, 0, 1, 0, 1],\n",
    "    [0, 0, 2, 1, 0],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffb1a37",
   "metadata": {
    "id": "8ffb1a37"
   },
   "source": [
    "Aceasta logica poate fi implementata usor, dar aceasta poate fi regasita (impreuna cu alte detalii interesante) in clasa [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) din scikit-learn.\n",
    "\n",
    "Intr-un obiect de acest tip putem incorpora si logica de preprocesare+tokenizare a textelor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8985a53b",
   "metadata": {
    "id": "8985a53b"
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Love fun fun Play Happy sad\",\n",
    "    \"love happy Love love tears\",\n",
    "    \"sad Tears tears\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3371bc70",
   "metadata": {
    "id": "3371bc70"
   },
   "source": [
    "Implementam o preprocesare rudimentara a acestor texte, transformam toate majusculele in litere mici, iar pentru tokenizare dam split dupa spatiu. In cazul in care lucrati cu texte mai complexe, in acesti pasi includem ce am discutat in laboratorul anterior (eliminare de punctuatie, tokenizare, eliminare de stopwords, lematizare, stemming etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "761b3404",
   "metadata": {
    "id": "761b3404",
    "outputId": "f1baf3fd-3817-4672-cf87-5be47d11f1a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'fun', 'fun', 'play', 'happy', 'sad']\n"
     ]
    }
   ],
   "source": [
    "def dummy_preprocess(text):\n",
    "    return text.lower()\n",
    "\n",
    "def dummy_tokenize(text):\n",
    "    return text.split(\" \")\n",
    "\n",
    "test_preprocess = dummy_preprocess(texts[0])\n",
    "test_tokens = dummy_tokenize(test_preprocess)\n",
    "print(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72a62d4f",
   "metadata": {
    "id": "72a62d4f",
    "outputId": "e377416f-168b-41a5-874a-8edc98c71c80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fun', 'happy', 'love', 'sad', 'tears']\n",
      "(3, 5)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 0]\n",
      " [0 1 1 0 1]\n",
      " [0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(\n",
    "    preprocessor=dummy_preprocess,  # metoda de preprocesare a textelor\n",
    "    tokenizer=dummy_tokenize,       # metoda de tokenizare\n",
    "    token_pattern=None,             # nu avem nevoie de acest argument intrucat avem propria metoda de tokenizare\n",
    "    max_features=5,                 # dimensiunea vocabularului care va fi determinat\n",
    "    binary=True,                    # BoW binar\n",
    ")\n",
    "\n",
    "# metoda `fit` este folosita in acest caz pentru a determina vocabularul\n",
    "# (pastrand cei mai frecventi `max_features` tokens din textele procesate)\n",
    "cv.fit(texts)\n",
    "\n",
    "print(sorted(list(cv.vocabulary_.keys())))\n",
    "\n",
    "# calculeaza reprezentarile textelor (feature vectors)\n",
    "features = cv.transform(texts)\n",
    "print(features.shape)\n",
    "print(type(features))\n",
    "print(features.toarray())  # convertim la numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84022c67",
   "metadata": {
    "id": "84022c67",
    "outputId": "abbf274c-5828-42eb-9aa6-5699750e36b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fun', 'happy', 'love', 'sad', 'tears']\n",
      "(3, 5)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[2 1 1 1 0]\n",
      " [0 1 3 0 1]\n",
      " [0 0 0 1 2]]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(\n",
    "    preprocessor=dummy_preprocess, \n",
    "    tokenizer=dummy_tokenize,       \n",
    "    token_pattern=None,             \n",
    "    max_features=5,                 \n",
    "    binary=False,                   # BoW bazat pe frecvente\n",
    ")\n",
    "\n",
    "cv.fit(texts)\n",
    "\n",
    "print(sorted(list(cv.vocabulary_.keys())))\n",
    "\n",
    "# calculeaza reprezentarile textelor (feature vectors)\n",
    "features = cv.transform(texts)\n",
    "print(features.shape)\n",
    "print(type(features))\n",
    "print(features.toarray())  # convertim la numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3e5de5",
   "metadata": {
    "id": "af3e5de5"
   },
   "source": [
    "O alternativa a acestor reprezentari este numararea n-gramelor.\n",
    "\n",
    "Sa luam spre exemplu fraza \"I am not happy\". Metoda de mai devreme ne reda reprezentarea acestei fraza ca o multime de cuvinte (indiferent de ordinea lor). Astfel, aceasta reprezentare indica prezenta cuvantului \"happy\", ceea ce ar face un model sa interpreteze textul intr-un cadru pozitiv. Dar vedem clar ca nu este aceasta situatia. Daca in loc de tokeni simpli (unigrame) am numara secvente de cate 2 tokeni consecutivi (bigrame), am putea identifica prezenta bigramei (\"not\", \"happy\") care intuim ca nu ar avea ce cauta intr-un text cu sentiment pozitiv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fe4c428",
   "metadata": {
    "id": "1fe4c428",
    "outputId": "f8baf33c-2bea-48f5-ac5a-3881572a9b4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fun fun', 'fun play', 'happy love', 'happy sad', 'love fun']\n",
      "(3, 5)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 0 1 1]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(\n",
    "    preprocessor=dummy_preprocess, \n",
    "    tokenizer=dummy_tokenize,       \n",
    "    token_pattern=None,             \n",
    "    max_features=5,                 \n",
    "    ngram_range=(2, 2),                   # construim un vocabular de bigrame\n",
    ")\n",
    "\n",
    "cv.fit(texts)\n",
    "\n",
    "print(sorted(list(cv.vocabulary_.keys())))\n",
    "\n",
    "# calculeaza reprezentarile textelor (feature vectors)\n",
    "features = cv.transform(texts)\n",
    "print(features.shape)\n",
    "print(type(features))\n",
    "print(features.toarray())  # convertim la numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83fd48c",
   "metadata": {
    "id": "d83fd48c"
   },
   "source": [
    "### 2. Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134e5179",
   "metadata": {
    "id": "134e5179"
   },
   "source": [
    "#### Standard Scaling\n",
    "\n",
    "Avand determinati feature vectorii se pune urmatoarea problema: stiind ca un feature reprezinta frecventa unui cuvant din vocabular in texte, iar pentru anumite cuvinte in mod natural aceasta frecventa este mai mare (Zipf's Law), faptul ca anumite feature-uri au valoare absoluta mai mare decat alta poate influenta diversi algoritmi de ML sa trateze acele feature-uri cu mai multa importanta (ceea ce de multe ori nu este corect).\n",
    "\n",
    "Prin Standard Scaling presupunem ca fiecare feature corespunde cate unei variabile aleatoare distribuita normal. Putem deci estima pentru fiecare astfel de variabila media si deviatia standard, apoi prin procesul de standardizare putem reduce valorile feature-ului corespunzator la o distributie normala de medie $0$ si varianta $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9791f91",
   "metadata": {
    "id": "e9791f91",
    "outputId": "a163e5e3-a5a2-4914-8ff2-e61aa266a609"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "media = [1.         0.         0.33333333]\n",
      "std = [0.81649658 0.81649658 1.24721913]\n",
      "[[ 0.         -1.22474487  1.33630621]\n",
      " [ 1.22474487  0.         -0.26726124]\n",
      " [-1.22474487  1.22474487 -1.06904497]]\n",
      "[[-2.44948974  1.22474487 -0.26726124]]\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array([[1, -1, 2], [2, 0, 0], [0, 1, -1]], dtype=np.float64)\n",
    "x_test = np.array([[-1, 1, 0]], dtype=np.float64)\n",
    " \n",
    "# clasa folosita pentru standardizare\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# calculam media si deviatia standard pentru feature vectorii de antrenare\n",
    "scaler.fit(x_train)\n",
    "\n",
    "print('media =', scaler.mean_)  \n",
    "print('std =', scaler.scale_) \n",
    "\n",
    "# scalam vectorii de train\n",
    "scaled_x_train = scaler.transform(x_train)\n",
    "print(scaled_x_train)  \n",
    "\n",
    "# scalam vectorii de test\n",
    "# (folosim media si std determinate din train, datele de test nu sunt folosite pentru a determina hiperparametrii)\n",
    "scaled_x_test = scaler.transform(x_test)\n",
    "print(scaled_x_test)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35243cb",
   "metadata": {
    "id": "b35243cb"
   },
   "source": [
    "#### Normalizare L1 si L2\n",
    "\n",
    "Acest mod de normalizare este mai simplu intrucat nu mai necesita determinarea unor parametrii. Fiecare feature vector este impartit prin norma sa (L1 sau L2), obtinand astfel ca toti vectorii corespunzatoari textelor sa aiba norma 1.\n",
    "\n",
    "$$x{\\_}scaled_1 = \\frac{X}{\\mid\\mid X \\mid\\mid_1}, \\mid\\mid X \\mid\\mid_1 = \\sum_{i=1}^{i=n}\\mid x_i \\mid $$\n",
    "$$x{\\_}scaled_2 = \\frac{X}{\\mid\\mid X \\mid\\mid_2}, \\mid\\mid X \\mid\\mid_2 = \\sqrt{\\sum_{i=1}^{i=n} x_i ^ 2 }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47ad1e98",
   "metadata": {
    "id": "47ad1e98",
    "outputId": "cbc06736-4bc6-440d-e4d6-9df6966ef01b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25 -0.25  0.5 ]\n",
      " [ 1.    0.    0.  ]\n",
      " [ 0.    0.5  -0.5 ]]\n",
      "[[-0.5  0.5  0. ]]\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array([[1, -1, 2], [2, 0, 0], [0, 1, -1]], dtype=np.float64)\n",
    "x_test = np.array([[-1, 1, 0]], dtype=np.float64)\n",
    "\n",
    "# clasa folosita pentru normalizare\n",
    "scaler = preprocessing.Normalizer(norm='l1') \n",
    "# scaler = preprocessing.Normalizer(norm='l2')\n",
    "\n",
    "# `fit` aici nu face absolut nimic\n",
    "scaler.fit(x_train)\n",
    "\n",
    "# scalam vectorii de train\n",
    "scaled_x_train = scaler.transform(x_train)\n",
    "print(scaled_x_train)  \n",
    "\n",
    "# scalam vectorii de test\n",
    "scaled_x_test = scaler.transform(x_test)\n",
    "print(scaled_x_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641c1c37",
   "metadata": {
    "id": "641c1c37"
   },
   "source": [
    "### 3. Tf-Idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc92232",
   "metadata": {
    "id": "cbc92232"
   },
   "source": [
    "O alta metoda de normalizare a feature-urilor pentru reprezentarea Bag-of-Words este **Tf-Idf**.\n",
    "\n",
    "**Term Frequency (TF)** numara pentru un token si un document de cate ori apare tokenul respectiv in document.\n",
    "\n",
    "O varianta de calcul pentru un token $t$ si un document $d$ ar fi: $\\textrm{tf}(t, d) = log(1 + \\textrm{frecv}(t, d))$.\n",
    "\n",
    "**Inverse Document Frequency (IDF)** iluestreaza cat de comun sau rar este un cuvant intr-un intreg corpus (multime de texte). Astfel, cu cat aceasta valoare se apropie de $0$ tokenul respectiv este mai comun.\n",
    "\n",
    "Pentru un token $t$, o multime de $N$ documente $D$, putem calcula IDF astfel: $\\textrm{idf}(t, D) = \\log(\\frac{N}{|\\{d \\in D: t \\textrm{ apare in } d\\}|})$.\n",
    "\n",
    "**TF-IDF** combina cele doua formule:\n",
    "$$\\textrm{tfidf}(t, d, D) = \\textrm{tf}(t, d) \\cdot \\textrm{idf}(t, D)$$.\n",
    "\n",
    "Mai multe informatii [aici](https://ro.wikipedia.org/wiki/Tf%E2%80%93idf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53971e83",
   "metadata": {
    "id": "53971e83",
    "outputId": "f9e8d610-3e52-457b-8bcf-6b19c10c29e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fun', 'happy', 'love', 'sad', 'tears']\n",
      "(3, 5)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0.83513325 0.31757018 0.31757018 0.31757018 0.        ]\n",
      " [0.         0.30151134 0.90453403 0.         0.30151134]\n",
      " [0.         0.         0.         0.4472136  0.89442719]]\n"
     ]
    }
   ],
   "source": [
    "# exemplu TfidfVectorizer\n",
    "\n",
    "cv = TfidfVectorizer(\n",
    "    preprocessor=dummy_preprocess, \n",
    "    tokenizer=dummy_tokenize,       \n",
    "    token_pattern=None,             \n",
    "    max_features=5,\n",
    ")\n",
    "\n",
    "cv.fit(texts)\n",
    "\n",
    "print(sorted(list(cv.vocabulary_.keys())))\n",
    "\n",
    "# calculeaza reprezentarile textelor (feature vectors)\n",
    "features = cv.transform(texts)\n",
    "print(features.shape)\n",
    "print(type(features))\n",
    "print(features.toarray())  # convertim la numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b81a88",
   "metadata": {
    "id": "40b81a88"
   },
   "source": [
    "### 4. Clasificare folosind Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b788ead5",
   "metadata": {
    "id": "b788ead5",
    "outputId": "e7962242-96d8-4f0f-fcc9-a4089b6e3adf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 10) (800,)\n",
      "(200, 10) (200,)\n"
     ]
    }
   ],
   "source": [
    "# generam o problema \"dummy\" de clasificare:\n",
    "# - 1000 de feature vectori de dimensiune 10 (din care doar 5 sunt relevanti pentru clasificare)\n",
    "# - fiecarui vector ii este asociat un label (0, 1 sau 2)\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_classes=3, n_features=10, n_informative=5, n_redundant=5, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb2f3735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.11034814 -3.86226934 -0.36295286 ...  1.02135721 -0.33060606\n",
      "   2.08996181]\n",
      " [-8.05056557  7.25228053 -0.70065301 ... -4.08261186  2.62797128\n",
      "  -3.67162169]\n",
      " [ 2.32951996 -5.38389315 -0.95445163 ...  3.95833456 -1.39369402\n",
      "   3.0696981 ]\n",
      " ...\n",
      " [-1.34415796  0.32652807 -0.87804473 ...  1.01145146 -1.50750514\n",
      "  -1.32659081]\n",
      " [-0.70429406 -0.54462889 -1.37962565 ... -0.33286085  0.40972293\n",
      "  -0.20107233]\n",
      " [-1.68574079 -2.08971035 -1.57534412 ...  1.67920024  0.16346792\n",
      "   0.79579295]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a620a91",
   "metadata": {
    "id": "8a620a91"
   },
   "source": [
    "Vom folosi pentru clasificare un decision tree (mai multe detalii [aici](https://en.wikipedia.org/wiki/Decision_tree_learning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50b6a114",
   "metadata": {
    "id": "50b6a114"
   },
   "outputs": [],
   "source": [
    "# initializam un astfel de algoritm cu anumiti hiperparametri\n",
    "clf = DecisionTreeClassifier(max_depth=3, min_samples_split=5)\n",
    "\n",
    "# antrenam folosind datele de train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# folosim modelul pentru a prezice labelurile de test\n",
    "y_test_predict = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47f75d0",
   "metadata": {
    "id": "d47f75d0"
   },
   "source": [
    "### 5. Metrici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa802f",
   "metadata": {
    "id": "50aa802f"
   },
   "source": [
    "Avem mai multe metrici pe care le putem folosi pentru a determina performanta unui astfel de model. Pornim de la cazul simplu al unei probleme de clasificare binara (0/1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48d28a94",
   "metadata": {
    "id": "48d28a94"
   },
   "outputs": [],
   "source": [
    "y_true    = [0, 0, 0, 1, 1, 1, 1]\n",
    "y_predict = [0, 1, 0, 0, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2195e9eb",
   "metadata": {
    "id": "2195e9eb"
   },
   "source": [
    "Definim:\n",
    " * TP (true positive): numarul de labeluri 1 prezise corect\n",
    " * FP (false positive): numarul de labeluri 1 prezise incorect\n",
    " * TN (true negative): numarul de labeluri 0 prezise corect\n",
    " * FN (false negative): numarul de labeluri 0 prezise incorect\n",
    " \n",
    "Acuratetea se calculeaza dupa formula:\n",
    "$$\\frac{TP+TN}{TP+FP+TN+FN}$$\n",
    "\n",
    "Cu alte cuvinte cate labeluri au fost prezise corect din totalul de predictii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b101b4a6",
   "metadata": {
    "id": "b101b4a6",
    "outputId": "e970be4e-1343-4b22-81bf-a5c9763603bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_true, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6746be29",
   "metadata": {
    "id": "6746be29"
   },
   "source": [
    "Acuratetea nu este mereu metrica preferabila. In cazul unei probleme dezechilibrate (imbalanced) in care $99\\%$ din obiecte au labelul $0$ si doar $1\\%$ au labelul $1$, un clasificator care prezice doar labelul $0$ pentru toate exemplele ar avea acuratete $99\\%$! Daca problema ar fi fost detectare de spam, sau detectarea simptomelor asociate unui anumit tip de cancer, un astfel de model nu ar fi considerat unul performant.\n",
    "\n",
    "Introducem astfel alte cateva metrici:\n",
    " * P (precision): $\\frac{TP}{TP+FP}$ (dintre toate predictiile 1, cate sunt corecte?)\n",
    " * R (recall): $\\frac{TP}{TP+FN}$ (dintre toate obiectele din clasa 1, cate am reusit sa prezicem corect?)\n",
    " * F1: $\\frac{2PR}{P+R}$ (media armonica intre precision si recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fa6fa30",
   "metadata": {
    "id": "2fa6fa30",
    "outputId": "4cd51f21-34dc-4409-efbe-8841fb3c1ef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6666666666666666\n",
      "Recall: 0.5\n",
      "F1: 0.5714285714285715\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision:\", metrics.precision_score(y_true, y_predict))\n",
    "print(\"Recall:\", metrics.recall_score(y_true, y_predict))\n",
    "print(\"F1:\", metrics.f1_score(y_true, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62475112",
   "metadata": {
    "id": "62475112"
   },
   "source": [
    "In cazul problemelor de clasificare de tip multi-class (mai mult de 2 clase). Acuratetea pastreaza acelasi sens de \"cate predictii sunt corecte din totalul predictiilor\".\n",
    "\n",
    "Precision, Recall si F1 se calculeaza intr-o maniera one-vs-rest. Astfel fixand o anumita clasa, transformam labelurile in $1$ daca labelul original corespunde clasei fixate si $0$ altfel. Putem acum calcula aceste metrici in cazul binar. Rezultatele obtinute pentru fiecare astfel de fixare sunt acumulate si rezultatul final este media acestora (calculata ca medie aritmetica - macro, sau ca medie ponderata - weighted in care ponderile sunt date de numarul de exemple din clasa respectiva)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a83260f9",
   "metadata": {
    "id": "a83260f9",
    "outputId": "bf4ad367-4025-4511-8e64-4baca9d14cf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7\n",
      "F1 (macro): 0.6935607271177741\n",
      "F1 (weighted): 0.6932996310177518\n"
     ]
    }
   ],
   "source": [
    "# calculam acuratete si F1 pentru clasificarea de mai devreme\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_test_predict))\n",
    "print(\"F1 (macro):\", metrics.f1_score(y_test, y_test_predict, average=\"macro\"))\n",
    "print(\"F1 (weighted):\", metrics.f1_score(y_test, y_test_predict, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "707cad42",
   "metadata": {
    "id": "707cad42",
    "outputId": "9b63015e-01df-4dba-ee4a-311280633d4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.74      0.76        68\n",
      "           1       0.67      0.86      0.75        65\n",
      "           2       0.65      0.51      0.57        67\n",
      "\n",
      "    accuracy                           0.70       200\n",
      "   macro avg       0.70      0.70      0.69       200\n",
      "weighted avg       0.70      0.70      0.69       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b30fd0",
   "metadata": {
    "id": "67b30fd0"
   },
   "source": [
    "O vizualizare utila este si matricea de confuzie in care numaram $A[i][j]$ cate obiecte din clasa $i$ au fost prezise ca facand parte din clasa $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d922431f",
   "metadata": {
    "id": "d922431f",
    "outputId": "4defa47b-0626-458f-dbf7-c92f11e49d69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50  7 11]\n",
      " [ 2 56  7]\n",
      " [12 21 34]]\n"
     ]
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(y_test, y_test_predict)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9760e755",
   "metadata": {
    "id": "9760e755",
    "outputId": "cc63d7cf-6759-4220-ad59-6ec9ad200f81"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e9b02d",
   "metadata": {
    "id": "78e9b02d"
   },
   "source": [
    "### 6. Importanta feature-urilor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d893d1dd",
   "metadata": {
    "id": "d893d1dd"
   },
   "source": [
    "Intuitiv anumite feature-uri sunt mai importante decat altele cand vine vorba de clasificare. Prezenta cuvantului \"happy\" sau \"sad\" ne poate oferi informatii despre sentimentul unui text, pe cand cuvantul \"car\", nu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0b88cc5",
   "metadata": {
    "id": "e0b88cc5",
    "outputId": "b4e8d50d-a950-4ffd-93b2-7dab27f9181a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: 0.11837956006934854\n",
      "Feature: 1, Score: 0.0\n",
      "Feature: 2, Score: 0.4172424563910119\n",
      "Feature: 3, Score: 0.039030730202152863\n",
      "Feature: 4, Score: 0.0\n",
      "Feature: 5, Score: 0.21084131505634365\n",
      "Feature: 6, Score: 0.0\n",
      "Feature: 7, Score: 0.04661388817258808\n",
      "Feature: 8, Score: 0.06021961536405184\n",
      "Feature: 9, Score: 0.10767243474450318\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAST0lEQVR4nO3df4xd513n8fcHZ51CC91ARkLYTsYtLuBSSNDgslSEFU0aV0F2pW2Fi4oCqmQVxRA2rBZ3QankqlJaVgX+MNtY1CvEEkxI+GO0HdYU2iIhlHQmTWixg9WJG+IxRR3qbAvbEsfJd/+YY3R7GWeOPXfmOs+8X9LI53nO89z7PYrzmePzM1WFJKld3zTuAiRJa8ugl6TGGfSS1DiDXpIaZ9BLUuOuGXcBw66//vqanJwcdxmS9LLy2GOP/WNVTSy37qoL+snJSebm5sZdhiS9rCT5u0ut89CNJDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ17qq7M1ZXZvLgx9b8O56+7441/w5Jo+cevSQ1zqCXpMYZ9JLUOINekhrXK+iT7E5yKsl8koMvMe4/JakkUwN97+3mnUpy+yiKliT1t+JVN0k2AYeB24AFYDbJdFWdHBr3rcDdwKMDfTuBfcDrge8C/izJ66rqhdFtgiTppfTZo98FzFfV6ao6DxwD9i4z7v3AB4F/GejbCxyrqueq6gvAfPd5kqR10ifotwBnBtoLXd+/SvJDwLaqGr6Ye8W53fz9SeaSzC0uLvYqXJLUz6pPxib5JuDDwC9f6WdU1ZGqmqqqqYmJZV95KEm6Qn3ujD0LbBtob+36LvpW4PuBTyUB+E5gOsmeHnMlSWuszx79LLAjyfYkm1k6uTp9cWVVfaWqrq+qyaqaBB4B9lTVXDduX5Jrk2wHdgCfHvlWSJIuacU9+qq6kOQAcBzYBBytqhNJDgFzVTX9EnNPJHkQOAlcAO7yihtJWl+9HmpWVTPAzFDfvZcY+x+H2h8APnCF9UmSVsk7YyWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjesV9El2JzmVZD7JwWXWvyfJ55I8keQvk+zs+ieTfL3rfyLJR0a9AZKkl7biG6aSbAIOA7cBC8BskumqOjkw7IGq+kg3fg/wYWB3t+6pqrpppFVLknrrs0e/C5ivqtNVdR44BuwdHFBVXx1ovhKo0ZUoSVqNPkG/BTgz0F7o+r5BkruSPAV8CPjFgVXbkzye5C+S/NhyX5Bkf5K5JHOLi4uXUb4kaSUjOxlbVYer6rXArwC/1nV/Ebihqm4G7gEeSPJty8w9UlVTVTU1MTExqpIkSfQL+rPAtoH21q7vUo4BbwOoqueq6svd8mPAU8DrrqhSSdIV6RP0s8COJNuTbAb2AdODA5LsGGjeAXy+65/oTuaS5DXADuD0KAqXJPWz4lU3VXUhyQHgOLAJOFpVJ5IcAuaqaho4kORW4HngWeDObvotwKEkzwMvAu+pqnNrsSGSpOWtGPQAVTUDzAz13TuwfPcl5j0MPLyaAiVJq+OdsZLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxvUK+iS7k5xKMp/k4DLr35Pkc0meSPKXSXYOrHtvN+9UkttHWbwkaWUrBn33ztfDwFuBncA7B4O880BVvaGqbgI+BHy4m7uTpXfMvh7YDfz2xXfISpLWR589+l3AfFWdrqrzwDFg7+CAqvrqQPOVQHXLe4FjVfVcVX0BmO8+T5K0Tvq8M3YLcGagvQC8cXhQkruAe4DNwE8MzH1kaO6WZebuB/YD3HDDDX3qliT1NLKTsVV1uKpeC/wK8GuXOfdIVU1V1dTExMSoSpIk0S/ozwLbBtpbu75LOQa87QrnSpJGrE/QzwI7kmxPspmlk6vTgwOS7Bho3gF8vlueBvYluTbJdmAH8OnVly1J6mvFY/RVdSHJAeA4sAk4WlUnkhwC5qpqGjiQ5FbgeeBZ4M5u7okkDwIngQvAXVX1whptiyRpGX1OxlJVM8DMUN+9A8t3v8TcDwAfuNICJUmr452xktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TG9Qr6JLuTnEoyn+TgMuvvSXIyyWeT/HmSGwfWvZDkie5neniuJGltrfiGqSSbgMPAbcACMJtkuqpODgx7HJiqqq8l+XngQ8BPdeu+XlU3jbZsSVJfffbodwHzVXW6qs4Dx4C9gwOq6pNV9bWu+QiwdbRlSpKuVJ+g3wKcGWgvdH2X8m7gTwbar0gyl+SRJG9bbkKS/d2YucXFxR4lSZL66vVy8L6SvAuYAn58oPvGqjqb5DXAJ5J8rqqeGpxXVUeAIwBTU1M1ypokaaPrs0d/Ftg20N7a9X2DJLcCvwrsqarnLvZX1dnuz9PAp4CbV1GvJOky9Qn6WWBHku1JNgP7gG+4eibJzcD9LIX8lwb6r0tybbd8PfAmYPAkriRpja146KaqLiQ5ABwHNgFHq+pEkkPAXFVNA78OvAr4oyQAz1TVHuD7gPuTvMjSL5X7hq7WkSStsV7H6KtqBpgZ6rt3YPnWS8z7K+ANqylQkrQ63hkrSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUuJG+YUraSCYPfmzNv+Pp++5Y8+9Q+9yjl6TG9Qr6JLuTnEoyn+TgMuvvSXIyyWeT/HmSGwfW3Znk893PnaMsXpK0shWDPskm4DDwVmAn8M4kO4eGPQ5MVdUPAA8BH+rmfjvwPuCNwC7gfUmuG135kqSV9Nmj3wXMV9XpqjoPHAP2Dg6oqk9W1de65iMsvUAc4Hbg41V1rqqeBT4O7B5N6ZKkPvoE/RbgzEB7oeu7lHcDf3KFcyVJIzbSq26SvAuYAn78MuftB/YD3HDDDaMsSZI2vD579GeBbQPtrV3fN0hyK/CrwJ6qeu5y5lbVkaqaqqqpiYmJvrVLknroE/SzwI4k25NsBvYB04MDktwM3M9SyH9pYNVx4C1JrutOwr6l65MkrZMVD91U1YUkB1gK6E3A0ao6keQQMFdV08CvA68C/igJwDNVtaeqziV5P0u/LAAOVdW5NdkSSdKyeh2jr6oZYGao796B5VtfYu5R4OiVFihJWh3vjJWkxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNa5X0CfZneRUkvkkB5dZf0uSzyS5kOTtQ+teSPJE9zM9PFeStLZWfMNUkk3AYeA2YAGYTTJdVScHhj0D/CzwX5b5iK9X1U2rL1WSdCX6vEpwFzBfVacBkhwD9gL/GvRV9XS37sU1qFGStAp9Dt1sAc4MtBe6vr5ekWQuySNJ3rbcgCT7uzFzi4uLl/HRkqSVrMfJ2Buragr4aeA3k7x2eEBVHamqqaqampiYWIeSJGnj6BP0Z4FtA+2tXV8vVXW2+/M08Cng5suoT5K0Sn2CfhbYkWR7ks3APqDX1TNJrktybbd8PfAmBo7tS5LW3opBX1UXgAPAceBJ4MGqOpHkUJI9AEl+OMkC8A7g/iQnuunfB8wl+Wvgk8B9Q1frSJLWWJ+rbqiqGWBmqO/egeVZlg7pDM/7K+ANq6xRkrQK3hkrSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXG9HoHwcjJ58GNr/h1P33fHmn+HJI2Ke/SS1DiDXpIaZ9BLUuMMeklqXHMnYyVpLbycL/TotUefZHeSU0nmkxxcZv0tST6T5EKStw+tuzPJ57ufO0dVuCSpnxWDPskm4DDwVmAn8M4kO4eGPQP8LPDA0NxvB94HvBHYBbwvyXWrL1uS1FefPfpdwHxVna6q88AxYO/ggKp6uqo+C7w4NPd24ONVda6qngU+DuweQd2SpJ76BP0W4MxAe6Hr66PX3CT7k8wlmVtcXOz50ZKkPq6Kq26q6khVTVXV1MTExLjLkaSm9An6s8C2gfbWrq+P1cyVJI1An6CfBXYk2Z5kM7APmO75+ceBtyS5rjsJ+5auT5K0TlYM+qq6ABxgKaCfBB6sqhNJDiXZA5Dkh5MsAO8A7k9yopt7Dng/S78sZoFDXZ8kaZ30umGqqmaAmaG+eweWZ1k6LLPc3KPA0VXUKElahaviZKwkae0Y9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY3r9ZhiSboaTB782Jp/x9P33bHm37He3KOXpMb1Cvoku5OcSjKf5OAy669N8ofd+keTTHb9k0m+nuSJ7ucjI65fkrSCFQ/dJNkEHAZuAxaA2STTVXVyYNi7gWer6ruT7AM+CPxUt+6pqrpptGVLkvrqs0e/C5ivqtNVdR44BuwdGrMX+N1u+SHgzUkyujIlSVeqT9BvAc4MtBe6vmXHdC8T/wrwHd267UkeT/IXSX5suS9Isj/JXJK5xcXFy9oASdJLW+uTsV8Ebqiqm4F7gAeSfNvwoKo6UlVTVTU1MTGxxiVJ0sbS5/LKs8C2gfbWrm+5MQtJrgFeDXy5qgp4DqCqHkvyFPA6YG61hUsaDy9xfPnps0c/C+xIsj3JZmAfMD00Zhq4s1t+O/CJqqokE93JXJK8BtgBnB5N6ZKkPlbco6+qC0kOAMeBTcDRqjqR5BAwV1XTwEeB30syD5xj6ZcBwC3AoSTPAy8C76mqc2uxIZKk5fW6M7aqZoCZob57B5b/BXjHMvMeBh5eZY26yvlPeenq5p2xktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TG9Qr6JLuTnEoyn+TgMuuvTfKH3fpHk0wOrHtv138qye0jrF2S1MOKQd+98/Uw8FZgJ/DOJDuHhr0beLaqvhv4DeCD3dydLL1W8PXAbuC3L75DVpK0Pvrs0e8C5qvqdFWdB44Be4fG7AV+t1t+CHhzknT9x6rquar6AjDffZ4kaZ30eWfsFuDMQHsBeOOlxnQvE/8K8B1d/yNDc7cMf0GS/cD+rvnPSU71qn40rgf+8XIm5INrVMn6ellt9wi/+7K3e5zc7pG4rG1/Gf89v/FSK3q9HHytVdUR4Mg4vjvJXFVNjeO7x8nt3lg26nbDxt72i/ocujkLbBtob+36lh2T5Brg1cCXe86VJK2hPkE/C+xIsj3JZpZOrk4PjZkG7uyW3w58oqqq69/XXZWzHdgBfHo0pUuS+ljx0E13zP0AcBzYBBytqhNJDgFzVTUNfBT4vSTzwDmWfhnQjXsQOAlcAO6qqhfWaFuu1FgOGV0F3O6NZaNuN2zsbQcgSzvekqRWeWesJDXOoJekxm3ooF/p0Q4tSrItySeTnExyIsnd465pPSXZlOTxJP973LWslyT/PslDSf42yZNJ/sO4a1oPSf5z93f8b5L8QZJXjLumcdmwQd/z0Q4tugD8clXtBH4EuGuDbPdFdwNPjruIdfZbwP+pqu8FfpANsP1JtgC/CExV1fezdCHJvvFWNT4bNujp92iH5lTVF6vqM93yP7H0P/2/uVu5RUm2AncAvzPuWtZLklcDt7B0ZRxVdb6q/u9Yi1o/1wDf3N3b8y3A34+5nrHZyEG/3KMdNkTgXdQ9ZfRm4NExl7JefhP4r8CLY65jPW0HFoH/2R2y+p0krxx3UWutqs4C/x14Bvgi8JWq+tPxVjU+GznoN7QkrwIeBn6pqr467nrWWpKfBL5UVY+Nu5Z1dg3wQ8D/qKqbgf8HNH8+Ksl1LP0LfTvwXcArk7xrvFWNz0YO+g37eIYk/46lkP/9qvrjcdezTt4E7EnyNEuH6X4iyf8ab0nrYgFYqKqL/2p7iKXgb92twBeqarGqngf+GPjRMdc0Nhs56Ps82qE53eOjPwo8WVUfHnc966Wq3ltVW6tqkqX/1p+oqub38KrqH4AzSb6n63ozS3eqt+4Z4EeSfEv3d/7NbICT0JdyVTy9chwu9WiHMZe1Ht4E/AzwuSRPdH3/rapmxleS1tgvAL/f7dCcBn5uzPWsuap6NMlDwGdYutLscTbwoxB8BIIkNW4jH7qRpA3BoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mN+/+X2FFvWgDpHQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pentru decision tree avem urmatoare metoda pentru a extrage aceasta importanta\n",
    "importance = clf.feature_importances_\n",
    "for i,v in enumerate(importance):\n",
    "    print(f\"Feature: {i}, Score: {v}\")\n",
    "\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6056deb9",
   "metadata": {
    "id": "6056deb9"
   },
   "source": [
    "Pentru alte modele exista diverse alte moduri de a extrage importanta feature-urilor (ex. logistic regression, SVM linear, Random Forest etc.), insa pentru altele este mult mai complicat sau chiar imposibil (ex. SVM cu kernel rbf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975369b0",
   "metadata": {
    "id": "975369b0"
   },
   "source": [
    "# TASK:\n",
    "\n",
    "### Deadline: 17 martie ora 23:59.\n",
    "### Formular pentru trimiterea temei: https://forms.gle/isPikzbiBdNm7AhA6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcb71b7",
   "metadata": {
    "id": "edcb71b7"
   },
   "source": [
    "Vom folosi urmatorul dataset: https://www.kaggle.com/rmisra/news-category-dataset (headline-uri de stiri etichetate conform unei liste de categorii).\n",
    "\n",
    "1. (optional) Intrucat setul de date contine multe categorii, puteti pastra exemplele din 4-5 clase (selectate de catre voi) si sa rezolvati problema de clasificare doar pentru aceste exemple\n",
    "2. Incercati mai multe metode de preprocesare si tokenizare a textelor pentru a obtine reprezentari de tip Bag-of-Words (stergeti/nu stergeti stop words, lematizati sau aplicati stemming, pastrati sau eliminati punctuatia, normalizati folosind standardizare, L1, L2 sau Tf-Idf). Implementati **3** astfel de combinatii.\n",
    "3. Impartiti setul de date in 80% train, 20% test, iar pentru fiecare metoda de preprocesare antrenati un model ales de voi (diferit de Decision Tree) pe datele de train\n",
    "4. Evaluati modelul pe datele de test, determinand acuratete, precizie, recall, f1, si stabiliti care metoda de procesare a textelor a adus rezultate mai bune.\n",
    "5. Determinati pentru acest caz care au fost top 10 cele mai importante feature-uri (cuvinte).\n",
    "6. Folosind aceasta metoda de procesare a textelor, antrenati alte doua modele diferite la alegere si comparati performanta cu modelul original.\n",
    "7. Pentru cel mai bun model afisati metricile la nivel de clasa (classification report) si matricea de confuzie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec5e9bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import time\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from num2words import num2words\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "def read_json():\n",
    "# Opening JSON file\n",
    "    f = open('News_Category_Dataset_v2.json')\n",
    "\n",
    "    # returns JSON object as a dictionary\n",
    "    dataset = []\n",
    "    \n",
    "    x = f.readline()\n",
    "    while x != '':\n",
    "        dataset.append(json.loads(x))\n",
    "        x = f.readline()\n",
    "    \n",
    "    # Closing file\n",
    "    f.close()\n",
    "    \n",
    "    return dataset\n",
    "#     return [dict(item, elem='value') for item in dataset]\n",
    "\n",
    "\n",
    "def relevant_lists(news):\n",
    "    news_clean = [(item[\"category\"], item[\"headline\"]) for item in news]\n",
    "    \n",
    "    categories = []\n",
    "    for elem in news_clean:\n",
    "        if elem[0] not in categories:\n",
    "            categories.append(elem[0])\n",
    "            \n",
    "    news_per_category = Convert(news_clean)\n",
    "    \n",
    "    headlines = [element[1] for element in news_clean]\n",
    "    \n",
    "#     number_of_headlines = len(headlines)\n",
    "#     number_of_train = int(number_of_headlines*0.8)\n",
    "#     news_train = headlines[:number_of_train]\n",
    "#     news_test = headlines[number_of_train:]\n",
    "    \n",
    "    return news_clean, categories, news_per_category, headlines\n",
    "\n",
    "def limited_categories(news, number_of_categories):\n",
    "    news_clean = [(item[\"category\"], item[\"headline\"]) for item in news]\n",
    "    \n",
    "    categories = []\n",
    "    for elem in news_clean:\n",
    "        if elem[0] not in categories:\n",
    "            categories.append(elem[0])\n",
    "        if len(categories) == number_of_categories:\n",
    "            break\n",
    "            \n",
    "    news_per_category = convert_limited(news_clean, categories)\n",
    "    \n",
    "    headlines = []\n",
    "    for element in news_clean:\n",
    "        if element[0] in categories:\n",
    "            headlines.append(element[1])\n",
    "    \n",
    "    categories_classes = []\n",
    "    for element in news_clean:\n",
    "        if element[0] in categories:\n",
    "            categories_classes.append(element[0])\n",
    "    \n",
    "#     number_of_headlines = len(headlines)\n",
    "#     number_of_train = int(number_of_headlines*0.8)\n",
    "#     news_train = headlines[:number_of_train]\n",
    "#     news_test = headlines[number_of_train:]\n",
    "    \n",
    "    return news_clean, categories, news_per_category, headlines, np.array(categories_classes)\n",
    "            \n",
    "def Convert(tup):\n",
    "    di = {}\n",
    "    for a, b in tup:\n",
    "        if a in tags:\n",
    "            di.setdefault(a, []).append(b)\n",
    "    return di\n",
    "\n",
    "def convert_limited(tup, tags):\n",
    "    di = {}\n",
    "    for a, b in tup:\n",
    "        di.setdefault(a, []).append(b)\n",
    "    return di\n",
    "\n",
    "news = read_json()\n",
    "# news_clean, categories, news_per_category, headlines = relevant_lists(news)\n",
    "NUMBER_OF_CATEGORIES = 4\n",
    "news_clean, categories, news_per_category, headlines, category_classes = limited_categories(news, NUMBER_OF_CATEGORIES)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "en_stop_words = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "726f51a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessors\n",
    "\n",
    "def delete_stop_words(text):\n",
    "    return ' '.join([word for word in text.split(' ') if word not in en_stop_words])\n",
    "    \n",
    "\n",
    "def no_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "\n",
    "def number_to_words(text):\n",
    "    return ' '.join([num2words(word, lang=\"en\") if word.isdigit() else word for word in text.split()])\n",
    "\n",
    "\n",
    "def to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def delete_digits(text):\n",
    "    return re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text)\n",
    "\n",
    "\n",
    "# tokenizers\n",
    "\n",
    "def stemming(text):\n",
    "    return [stemmer.stem(str(word)) for word in nlp(text)]\n",
    "\n",
    "\n",
    "def lemmatize(text):\n",
    "#     return [word for review in reviews for word in nlp(review)]\n",
    "    return [word.lemma_ for word in nlp(text)]\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return [word.text for word in nlp(text)]\n",
    "\n",
    "\n",
    "def tokenize_stop_words(text):\n",
    "    return [word.text for word in nlp(text) if word.text not in en_stop_words]\n",
    "\n",
    "\n",
    "# ML bullshit\n",
    "\n",
    "def standardization(features):\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    \n",
    "    scaler.fit(features)\n",
    "    # scalam vectorii de train\n",
    "    scaled_x = scaler.transform(features)\n",
    "    \n",
    "    return scaled_x\n",
    "\n",
    "\n",
    "def normalization(features, norma = 'l1'):\n",
    "    scaler = preprocessing.Normalizer(norm=norma)\n",
    "\n",
    "    scaler.fit(features)\n",
    "    # scalam vectorii de train\n",
    "    scaled_x = scaler.transform(features)\n",
    "    \n",
    "    return scaled_x\n",
    "\n",
    "\n",
    "def tfidf_bog(list_texts, preprocess_function, tokenize_function):\n",
    "    start = time.time()\n",
    "    cv = TfidfVectorizer(\n",
    "        preprocessor=preprocess_function, \n",
    "        tokenizer=tokenize_function,       \n",
    "        token_pattern=None,      \n",
    "        max_features=100,                 \n",
    "        ngram_range=(2, 2),                   # construim un vocabular de bigrame\n",
    "    )\n",
    "\n",
    "    cv.fit(list_texts)\n",
    "\n",
    "    features = cv.transform(list_texts)\n",
    "    \n",
    "    print(time.time() - start)\n",
    "    return cv, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2651f893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cv1\n",
      "288.2896337509155\n",
      "Before cv2\n",
      "254.73317337036133\n",
      "Before cv3\n",
      "281.25472140312195\n"
     ]
    }
   ],
   "source": [
    "def preprocessing_1(text):\n",
    "    return number_to_words(no_punctuation(to_lower(text)))\n",
    "\n",
    "\n",
    "def preprocessing_2(text):\n",
    "    return delete_stop_words(no_punctuation(to_lower(text)))\n",
    "\n",
    "\n",
    "def preprocessing_3(text):\n",
    "    return delete_stop_words(delete_digits(to_lower(text)))\n",
    "\n",
    "\n",
    "def bog(list_texts, preprocess_function, tokenize_function):\n",
    "    start = time.time()\n",
    "    cv = CountVectorizer(     \n",
    "        preprocessor=preprocess_function, \n",
    "        tokenizer = tokenize_function,\n",
    "        token_pattern=None,        \n",
    "        max_features=100,                 \n",
    "        ngram_range=(2, 2),                   # construim un vocabular de bigrame\n",
    "    )\n",
    "\n",
    "    cv.fit(list_texts)\n",
    "    features = cv.transform(list_texts)\n",
    "    \n",
    "    print(time.time() - start)\n",
    "    return cv, features\n",
    "\n",
    "\n",
    "print(\"Before cv1\")\n",
    "cv1, features1 = bog(headlines, preprocessing_1, tokenize_stop_words)\n",
    "print(\"Before cv2\")\n",
    "cv2, features2_norm = tfidf_bog(headlines, preprocessing_2, lemmatize)\n",
    "print(\"Before cv3\")\n",
    "cv3, features3 = bog(headlines, preprocessing_3, stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f39ee3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features1_array = features1.toarray()\n",
    "features2_norm_array = features2_norm.toarray()\n",
    "features3_array = features3.toarray()\n",
    "\n",
    "\n",
    "features1_norm = standardization(features1_array)\n",
    "# features2_norm = cv2.transform(headlines)\n",
    "features3_norm = normalization(features3)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5999aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_data(features, classes):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, classes, test_size=0.2, random_state=42)\n",
    "    return (X_train, X_test, y_train, y_test)\n",
    "\n",
    "batches = [split_train_data(features1_norm, category_classes), split_train_data(features2_norm_array, category_classes), split_train_data(features3_norm, category_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c7f639c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for max_depth=1000:  10.929572820663452\n",
      "Accuracy: 0.5714285714285714\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.5\n",
      "F1: 0.5714285714285715\n",
      "F1 (macro): 0.2605224557052945\n",
      "F1 (weighted): 0.536502218941042\n",
      "Average of all metrics: 0.5177580806950243 \n",
      "\n",
      "Time for max_depth=1000:  10.780450820922852\n",
      "Accuracy: 0.5714285714285714\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.5\n",
      "F1: 0.5714285714285715\n",
      "F1 (macro): 0.26723935434826807\n",
      "F1 (weighted): 0.5410831945589012\n",
      "Average of all metrics: 0.5196410597384965 \n",
      "\n",
      "Time for max_depth=1000:  1.1250097751617432\n",
      "Accuracy: 0.5714285714285714\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.5\n",
      "F1: 0.5714285714285715\n",
      "F1 (macro): 0.3139925110605921\n",
      "F1 (weighted): 0.5635812572391722\n",
      "Average of all metrics: 0.5311829296372624 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def random_forest(X_train, y_train, X_test, max_depth = 100):\n",
    "    start = time.time()\n",
    "    rfc = RandomForestClassifier(max_depth=max_depth, random_state=42)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    y_test_predict = rfc.predict(X_test)\n",
    "    print(\"Time for max_depth={}: \".format(str(max_depth)), time.time()-start)\n",
    "    return (rfc, y_test_predict)\n",
    "\n",
    "\n",
    "def calculate_metrics(y_test, y_test_predict):\n",
    "    accuracy = metrics.accuracy_score(y_true, y_predict)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    precision = metrics.precision_score(y_true, y_predict)\n",
    "    print(\"Precision:\", precision)\n",
    "    recall = metrics.recall_score(y_true, y_predict)\n",
    "    print(\"Recall:\", recall)\n",
    "    f1 = metrics.f1_score(y_true, y_predict)\n",
    "    print(\"F1:\", f1)\n",
    "    f1_macro = metrics.f1_score(y_test, y_test_predict, average=\"macro\")\n",
    "    print(\"F1 (macro):\", f1_macro)\n",
    "    f1_weighted = metrics.f1_score(y_test, y_test_predict, average=\"weighted\")\n",
    "    print(\"F1 (weighted):\", f1_weighted)\n",
    "    metrics_average = (accuracy + precision + recall + f1 + f1_macro + f1_weighted) / 6\n",
    "    return (accuracy, precision, recall, f1, f1_macro, f1_weighted), metrics_average\n",
    "    \n",
    "\n",
    "rfc_batches = []\n",
    "best_metrics = (-1,0)\n",
    "i=0\n",
    "for batch in batches:\n",
    "    rfc_batch = random_forest(batch[0], batch[2], batch[1], 1000)  # X_train, y_train, X_test\n",
    "    rfc_batches.append(rfc_batch)\n",
    "    calculated_metrics, metrics_average = calculate_metrics(batch[3], rfc_batch[1])\n",
    "    print(\"Average of all metrics: {}\".format(metrics_average), \"\\n\")\n",
    "    if metrics_average > best_metrics[1]:\n",
    "        best_metrics = (i, metrics_average)\n",
    "    i +=1\n",
    "\n",
    "best_preprocess_model = (batches[best_metrics[0]], rfc_batches[best_metrics[0]])\n",
    "#     print(metrics.classification_report(batch[3], rfc_batch[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ceaf826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features for best model (3):\n",
      "[\"' cast\", 'britney spear', \"' movi\", \"she 's\", 'kati perri', 'star war', \"'s new\", \"' video\", 'box offic', \"' snl\"] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def importance_features(model):\n",
    "    importance = model.feature_importances_\n",
    "    feature_score = [(i,v) for i,v in enumerate(importance)]\n",
    "    feature_score.sort(key = lambda x:x[1], reverse = True)\n",
    "    top_10 = feature_score[0:10]\n",
    "#     print([v for (i,v) in top_10])\n",
    "#     plt.bar([x for x in range(len(importance))], importance)\n",
    "#     plt.show()\n",
    "    return importance, feature_score, top_10\n",
    "def best_features_all_batches(rfc_batches):\n",
    "    for i in range(len(rfc_batches)):\n",
    "        print(\"Top 10 features for batch {}:\".format(i + 1))\n",
    "        importance, feature_score, top_10 = importance_features(rfc_batches[i][0])\n",
    "        print([keys[i][index] for (index,v) in top_10], \"\\n\")\n",
    "    \n",
    "# best_features_all_batches(rfc_batches)\n",
    "\n",
    "keys = (list(cv1.vocabulary_.keys()), list(cv2.vocabulary_.keys()), list(cv3.vocabulary_.keys()))\n",
    "\n",
    "print(\"Top 10 features for best model ({}):\".format(best_metrics[0] + 1))\n",
    "importance, feature_score, top_10 = importance_features(best_preprocess_model[1][0])\n",
    "print([keys[best_metrics[0]][index] for (index,v) in top_10], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7bacc760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for SVC:  6.973179817199707\n",
      "Accuracy: 0.5714285714285714\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.5\n",
      "F1: 0.5714285714285715\n",
      "F1 (macro): 0.3113508171691057\n",
      "F1 (weighted): 0.5618830341954194\n",
      "Average of all metrics SVC: 0.5304596101480558 \n",
      "\n",
      "Time for LinearSVC:  20.376685857772827\n",
      "Accuracy: 0.5714285714285714\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.5\n",
      "F1: 0.5714285714285715\n",
      "F1 (macro): 0.3124869812127327\n",
      "F1 (weighted): 0.563003414135592\n",
      "Average of all metrics linearSVC: 0.5308357008120224 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def model_SVC(X_train, y_train, X_test):\n",
    "    start = time.time()\n",
    "    clf = make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto'))\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_test_predict = clf.predict(X_test)\n",
    "    print(\"Time for SVC: \", time.time()-start)\n",
    "    return (clf, y_test_predict)\n",
    "\n",
    "def model_linearSVC(X_train, y_train, X_test):\n",
    "    start = time.time()\n",
    "    clf = make_pipeline(StandardScaler(with_mean=False), LinearSVC(random_state=0, tol=1e-5, max_iter = 10000))\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_test_predict = clf.predict(X_test)\n",
    "    print(\"Time for LinearSVC: \", time.time()-start)\n",
    "    return (clf, y_test_predict)\n",
    "\n",
    "\n",
    "svc_batch = model_SVC(best_preprocess_model[0][0], best_preprocess_model[0][2], best_preprocess_model[0][1])  # X_train, y_train, X_test\n",
    "calculated_metrics_svc, metrics_average_svc = calculate_metrics(best_preprocess_model[0][3], svc_batch[1])\n",
    "print(\"Average of all metrics SVC: {}\".format(metrics_average_svc), \"\\n\")\n",
    "\n",
    "linearSVC_batch = model_linearSVC(best_preprocess_model[0][0], best_preprocess_model[0][2], best_preprocess_model[0][1])  # X_train, y_train, X_test\n",
    "calculated_metrics_linearSVC, metrics_average_linearSVC = calculate_metrics(best_preprocess_model[0][3], linearSVC_batch[1])\n",
    "print(\"Average of all metrics linearSVC: {}\".format(metrics_average_linearSVC), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b57e6ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "        CRIME       0.64      0.12      0.20       674\n",
      "ENTERTAINMENT       0.67      0.99      0.79      3236\n",
      "       IMPACT       0.48      0.02      0.03       663\n",
      "   WORLD NEWS       0.77      0.13      0.23       447\n",
      "\n",
      "     accuracy                           0.67      5020\n",
      "    macro avg       0.64      0.31      0.31      5020\n",
      " weighted avg       0.65      0.67      0.56      5020\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model = None\n",
    "if best_metrics[1] > metrics_average_svc:\n",
    "    best_model = best_preprocess_model\n",
    "elif metrics_average_svc > metrics_average_linearSVC:\n",
    "    best_model = (best_preprocess_model[0], svc_batch)\n",
    "else:\n",
    "    best_model = (best_preprocess_model[0], linearSVC_batch)\n",
    "\n",
    "print(metrics.classification_report(best_model[0][3], best_model[1][1]))\n",
    "cm = metrics.confusion_matrix(best_model[0][3], best_model[1][1])\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87a219d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
