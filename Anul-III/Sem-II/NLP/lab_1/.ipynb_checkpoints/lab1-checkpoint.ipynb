{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYN6xXrf0jdy"
   },
   "source": [
    "# Lab 1. Basic Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jl-E8ey90y3-"
   },
   "source": [
    "## Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibjQLLG9yCZS"
   },
   "source": [
    "Expresiile regulate. Utile pentru cautare si normalizare de texte - https://www.w3schools.com/python/python_regex.asp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "gFqSzXcNyPU7"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"\n",
    "Praise for The Rain in Portugal\n",
    " \n",
    "“Nothing in Billy Collins’s twelfth book . . . is exactly what readers might expect, and that’s the charm of this collection.”—The Washington Post\n",
    " \n",
    "“This new collection shows [Collins] at his finest. . . . Certain to please his large readership and a good place for readers new to Collins to begin.”—Library Journal. \n",
    " \n",
    "“Disarmingly playful and wistfully candid.”—Booklist\n",
    "Buy new:$38.65\n",
    "No Import Fees Deposit & $13.01 Shipping to Romania Details -12.3.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLNJrErsyXNN"
   },
   "source": [
    "Sterge toate caracterele diferite de litere mari si mici ale alfabetului englez, apoi normalizeaza toate secventele de caractere de tip spatii la 1 spatiu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 246,
     "status": "ok",
     "timestamp": 1644683724989,
     "user": {
      "displayName": "Bogdan Iordache",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64",
      "userId": "11937921055418047811"
     },
     "user_tz": -120
    },
    "id": "YP7lMkYPyW0H",
    "outputId": "1aaf8845-a230-4ca6-b06a-c893408e43b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Praise for The Rain in Portugal Nothing in Billy Collins s twelfth book is exactly what readers might expect and that s the charm of this collection The Washington Post This new collection shows Collins at his finest Certain to please his large readership and a good place for readers new to Collins to begin Library Journal Disarmingly playful and wistfully candid Booklist Buy new No Import Fees Deposit Shipping to Romania Details \n"
     ]
    }
   ],
   "source": [
    "cleaned_text = re.sub(\"[^A-Za-z]\", \" \", text)\n",
    "cleaned_text = re.sub(\"\\s+\", \" \", cleaned_text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LOnaRquyhlP"
   },
   "source": [
    "Le putem testa cel mai usor pe https://pythex.org/ . Aici cautam toate numerele float sau int, impreuna cu pozitiile si valorile lor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 248,
     "status": "ok",
     "timestamp": 1644684003604,
     "user": {
      "displayName": "Bogdan Iordache",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64",
      "userId": "11937921055418047811"
     },
     "user_tz": -120
    },
    "id": "1uJAKaNHyUS_",
    "outputId": "d94404da-f9e8-4fb8-c00f-8c95e28d59ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418 38.65\n",
      "450 13.01\n",
      "484 -12.3\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(\"[+-]?(\\d+\\.)?\\d+\")\n",
    "for matching in pattern.finditer(text):\n",
    "    print(matching.start(), matching.group())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQMO4b6u007D"
   },
   "source": [
    "## Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bI5ffSzvfJi8"
   },
   "source": [
    "Encodarea unui text poate varia, in functie de limba si este importanta. Python foloseste standardul 'utf8'. Urmatorul exemplu este preluat dintr-o subtitrare (.srt) din limba rusa, dar nu este encodat in utf8. Citindu-l obtinem urmatoarea eroare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "executionInfo": {
     "elapsed": 233,
     "status": "error",
     "timestamp": 1644678939085,
     "user": {
      "displayName": "Bogdan Iordache",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64",
      "userId": "11937921055418047811"
     },
     "user_tz": -120
    },
    "id": "W1F8_BTNfOK1",
    "outputId": "1486b3df-db44-4424-b8d8-ef19cb4fe330"
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xdd in position 34: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16048/3329779444.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'encoded_text.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python310\\lib\\codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;31m# decode input (taking the buffer into account)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m         \u001b[1;31m# keep undecoded input until the next call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xdd in position 34: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "with open('encoded_text.txt', \"r\") as fin:\n",
    "    content = fin.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60znqZ6wfP-s"
   },
   "source": [
    "Incercam sa detectam encoding-ul lui folosind chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "gvOUmRW2fRN1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: chardet in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1644678991060,
     "user": {
      "displayName": "Bogdan Iordache",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64",
      "userId": "11937921055418047811"
     },
     "user_tz": -120
    },
    "id": "pBBadYt8fXx8",
    "outputId": "8337c7c7-bd49-4889-9600-69399bde4512"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "windows-1251\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "with open('encoded_text.txt', \"rb\") as f:\n",
    "    rawdata = f.read()\n",
    "    result = chardet.detect(rawdata)\n",
    "    extracted_encoding = result['encoding']\n",
    "    print(extracted_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwjdvMNSfZTP"
   },
   "source": [
    "Reluam citirea si salvam continutul in format utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "OPBYzpfYfcF6"
   },
   "outputs": [],
   "source": [
    "with open('encoded_text.txt', 'r', encoding=extracted_encoding) as fin:\n",
    "    content = fin.read()\n",
    "with open('utf8_text.txt', 'w', encoding='utf8') as fout:\n",
    "    fout.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1644679004976,
     "user": {
      "displayName": "Bogdan Iordache",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64",
      "userId": "11937921055418047811"
     },
     "user_tz": -120
    },
    "id": "3ZwGqlQifzDk",
    "outputId": "d5330318-7461-46ab-b094-d1e19fc1fb28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "00:00:05,100 --> 00:00:10,860\n",
      "Это были тяжелые времена. Рим находился под господством коррумпированного Папы и сомнительных законов\n",
      "\n",
      "2\n",
      "00:00:10,960 --> 00:00:13,820\n",
      "игр власти и междоусобной борьбы\n",
      "\n",
      "3\n",
      "00:00:15,660 --> 00:00:20,310\n",
      "Синьоры начинали жестокие сражения с единственной целью – накопить состояние\n",
      "\n",
      "4\n",
      "00:00:21,640 --> 00:00:24,950\n",
      "А тем временем, простые люди ели не каждый день\n",
      "\n",
      "5\n",
      "00:00:30,090 --> 00:00:36,040\n",
      "Любовь была темой для поэтов, но редко упоминалась в свадебных клятвах\n",
      "\n",
      "6\n",
      "00:00:36,940 --> 00:00:42,140\n",
      "Женщин отдавали в жены мужчинам, которых они едва знали, не говоря уже о любви\n",
      "\n",
      "7\n",
      "00:00:43,040 --> 00:00:47,980\n",
      "В этом мире, жестоком и несправедливом, я повстречала двух молодых людей\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('utf8_text.txt', \"r\") as fin:\n",
    "    content = fin.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEC0t_4T05S2"
   },
   "source": [
    "## Non-standard files (PDF, Word, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kT-3FzVo0QKT"
   },
   "source": [
    "Putem citi texte din documente word folosind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: docx2txt in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (0.8)\n"
     ]
    }
   ],
   "source": [
    "! pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 282,
     "status": "ok",
     "timestamp": 1644684536901,
     "user": {
      "displayName": "Bogdan Iordache",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64",
      "userId": "11937921055418047811"
     },
     "user_tz": -120
    },
    "id": "r9aPabVw0XBH",
    "outputId": "76ff3ec9-247c-4efd-b460-28c31dbdb652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work.\n",
      "\n",
      "\n",
      "\n",
      "These instructions illustrate all major features of Beautiful Soup 4, with examples. I show you what the library is good for, how it works, how to use it, how to make it do what you want, and what to do when it violates your expectations.\n",
      "\n",
      "\n",
      "\n",
      "This document covers Beautiful Soup version 4.10.0. The examples in this documentation were written for Python 3.8.\n",
      "\n",
      "\n",
      "\n",
      "You might be looking for the documentation for Beautiful Soup 3. If so, you should know that Beautiful Soup 3 is no longer being developed and that all support for it was dropped on December 31, 2020. If you want to learn about the differences between Beautiful Soup 3 and Beautiful Soup 4, see Porting code to BS4.\n",
      "\n",
      "\n",
      "\n",
      "This documentation has been translated into other languages by Beautiful Soup users:\n"
     ]
    }
   ],
   "source": [
    "import docx2txt\n",
    "my_text = docx2txt.process(\"soup.docx\")\n",
    "print(my_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYv-2JYZ1YD3"
   },
   "source": [
    "Sau pdf-uri care sunt salvate ca texte (nu poze):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "uZHrdF851Z3q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (0.6.0)\n",
      "Requirement already satisfied: pdfminer.six==20211012 in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (from pdfplumber) (20211012)\n",
      "Requirement already satisfied: Pillow>=8.4 in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (from pdfplumber) (9.0.0)\n",
      "Requirement already satisfied: Wand>=0.6.7 in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (from pdfplumber) (0.6.7)\n",
      "Requirement already satisfied: chardet in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (from pdfminer.six==20211012->pdfplumber) (4.0.0)\n",
      "Requirement already satisfied: cryptography in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (from pdfminer.six==20211012->pdfplumber) (36.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (from cryptography->pdfminer.six==20211012->pdfplumber) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (from cffi>=1.12->cryptography->pdfminer.six==20211012->pdfplumber) (2.21)\n"
     ]
    }
   ],
   "source": [
    "! pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 952,
     "status": "ok",
     "timestamp": 1644684520795,
     "user": {
      "displayName": "Bogdan Iordache",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64",
      "userId": "11937921055418047811"
     },
     "user_tz": -120
    },
    "id": "A_x4wZpQ1hh8",
    "outputId": "e937a5ee-fb74-4c04-bfb2-68ad259bdf1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite \n",
      "parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly \n",
      "saves programmers hours or days of work. \n",
      " \n",
      "These instructions illustrate all major features of Beautiful Soup 4, with examples. I show you what the \n",
      "library is good for, how it works, how to use it, how to make it do what you want, and what to do when \n",
      "it violates your expectations. \n",
      " \n",
      "This document covers Beautiful Soup version 4.10.0. The examples in this documentation were written \n",
      "for Python 3.8. \n",
      " \n",
      "You might be looking for the documentation for Beautiful Soup 3. If so, you should know that Beautiful \n",
      "Soup 3 is no longer being developed and that all support for it was dropped on December 31, 2020. If \n",
      "you want to learn about the differences between Beautiful Soup 3 and Beautiful Soup 4, see Porting \n",
      "code to BS4. \n",
      " \n",
      "This documentation has been translated into other languages by Beautiful Soup users: \n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "with pdfplumber.open('soup.pdf') as pdf:\n",
    "    for page in pdf.pages:\n",
    "        print(page.extract_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ix-cDK_1FrO"
   },
   "source": [
    "## Web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZ5jU3-C9Xrw"
   },
   "source": [
    "Prin scraping ne referim la o multime de metode prin care putem descarca date nestructurate din mediul web, pe care le putem apoi procesa si stoca sub forma structurata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kz0-C3BJj9V_"
   },
   "source": [
    "Ca prim exemplu de scraping vom incerca urmatorul task: pornind de la site-ul de programare competitiva \"infoarena.ro\" dorim pentru un utilizator sa descarcam informatii despre toate submisiile efectuate de acesta.\n",
    "\n",
    "Exemplu pagina de submisii: https://www.infoarena.ro/monitor?user=iordache.bogdan\n",
    "\n",
    "Pentru a realiza un request care sa intoarca continutul paginii putem folosi biblioteca \"requests\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (2.26.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (from requests) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (from requests) (3.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Kp2mSeC_9CCu"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_submissions_page(user):\n",
    "    return requests.get(f\"https://www.infoarena.ro/monitor?user={user}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "13W4i-G3k7hl"
   },
   "outputs": [],
   "source": [
    "html = get_submissions_page(\"iordache.bogdan\").content\n",
    "# print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aSHQWnilIOR"
   },
   "source": [
    "Observam ca folosind metoda de mai sus putem descarca intreg continutul HTML al paginii. Pentru a extrage informatii utile trebuie sa parsam acest continut. Vom folosi biblioteca [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bs4 in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (from bs4) (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "a1NrQndSlG4g"
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "\n",
    "def parse_html(html):\n",
    "    return bs4.BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyKYAfF7l3pO"
   },
   "source": [
    "Avand continutul parsat, putem determina acum cate submisii are in total acest utilizator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 258,
     "status": "ok",
     "timestamp": 1644680792685,
     "user": {
      "displayName": "Bogdan Iordache",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64",
      "userId": "11937921055418047811"
     },
     "user_tz": -120
    },
    "id": "UB0sUnx0ltvH",
    "outputId": "e5cd8d9e-0ac0-4f55-d14a-8e0b05ea6e21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (5027 rezultate)\n",
      "5027\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "soup = parse_html(html)\n",
    "\n",
    "# cautam un span care are clasa \"count\", in acest span se afla numarul de submisii\n",
    "submission_count_text = soup.find(\"span\", class_=\"count\").text\n",
    "print(submission_count_text)\n",
    "submission_count = int(re.search(r\"\\d+\", submission_count_text).group())\n",
    "print(submission_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Obu7nG4HnSIJ"
   },
   "source": [
    "Observam ca aceste submisii sunt impartite in mai multe pagini (paginarea rezultatelor). Observam ca link-ul urmator: https://www.infoarena.ro/monitor?user=iordache.bogdan&display_entries=250&first_entry=100 ne returneaza 250 de submisii, incepand cu submisia cu numarul 100. Modificam metoda get_submissions_page astfel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "xwKHy-zGnQ27"
   },
   "outputs": [],
   "source": [
    "def get_submissions_page(user, display_entries=None, first_entry=None):\n",
    "    req_string = f\"https://www.infoarena.ro/monitor?user={user}\"\n",
    "    if display_entries is not None:\n",
    "        req_string += f\"&display_entries={display_entries}\"\n",
    "    if first_entry is not None:\n",
    "        req_string += f\"&first_entry={first_entry}\"\n",
    "\n",
    "    return requests.get(req_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pv9ylMxPoTh4"
   },
   "source": [
    "Implementam o functie care returneaza informatii despre toate submisiile unui utilizator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "sLG0u9POonXT"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pdb\n",
    "\n",
    "def scrape_submissions(user):\n",
    "    # determinam numarul total de submisii\n",
    "    html = get_submissions_page(user).content\n",
    "    soup = parse_html(html)\n",
    "    submission_count_text = soup.find(\"span\", class_=\"count\").text\n",
    "    submission_count = int(re.search(r\"\\d+\", submission_count_text).group())\n",
    "\n",
    "    # vom salva in acest dictionar datele despre submisiile extrase, structura aceasta\n",
    "    # ne va ajuta ulterior sa construim un tabel (dataframe) folosind pandas\n",
    "    d = {\n",
    "        \"id\": [],\n",
    "        \"problema\": [],\n",
    "        \"url_problema\": [],\n",
    "        \"url_sursa\": [],\n",
    "        \"data\": [],\n",
    "        \"puncte\": [],\n",
    "    }\n",
    "\n",
    "    # accesam pagini cu submisii in grupuri de 250\n",
    "    for first_entry in tqdm(range(0, submission_count, 250)):\n",
    "        html = get_submissions_page(user, display_entries=250, first_entry=first_entry).content\n",
    "        soup = parse_html(html)\n",
    "\n",
    "        # selectam toate liniile de tabel (tr)\n",
    "        lines = soup.select(\"table.monitor tbody tr\")\n",
    "\n",
    "        for line in lines:\n",
    "            # selectam celulele de pe aceasta linie\n",
    "            cells = [cell for cell in line.select(\"td\")]\n",
    "\n",
    "            # extragem link-urile pentru problema si codul sursa\n",
    "            try:\n",
    "                url_problema = cells[2].select_one(\"a\")[\"href\"]\n",
    "                url_sursa = cells[4].select_one(\"a\")[\"href\"]\n",
    "            except Exception:  # daca vreun link nu exista ignoram linia\n",
    "                continue\n",
    "            \n",
    "            d[\"id\"].append(cells[0].text)\n",
    "            d[\"problema\"].append(cells[2].text)\n",
    "            d[\"url_problema\"].append(url_problema)\n",
    "            d[\"url_sursa\"].append(url_sursa)\n",
    "            d[\"data\"].append(cells[5].text)\n",
    "\n",
    "            try:\n",
    "                puncte = int(re.search(r\"\\d+\", cells[6].text).group())\n",
    "            except Exception:\n",
    "                puncte = 0\n",
    "            d[\"puncte\"].append(puncte)\n",
    "\n",
    "    return pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47013,
     "status": "ok",
     "timestamp": 1644683262587,
     "user": {
      "displayName": "Bogdan Iordache",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64",
      "userId": "11937921055418047811"
     },
     "user_tz": -120
    },
    "id": "W9Pf0rtfrL_w",
    "outputId": "c6bc6bb7-15d2-4220-9192-e6a1a0bb826b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 21/21 [00:43<00:00,  2.08s/it]\n"
     ]
    }
   ],
   "source": [
    "df_submissions = scrape_submissions(\"iordache.bogdan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1644683266483,
     "user": {
      "displayName": "Bogdan Iordache",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64",
      "userId": "11937921055418047811"
     },
     "user_tz": -120
    },
    "id": "LwOII21wtFKF",
    "outputId": "4c315a21-7994-46e4-a6a1-19083f4aea61"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>problema</th>\n",
       "      <th>url_problema</th>\n",
       "      <th>url_sursa</th>\n",
       "      <th>data</th>\n",
       "      <th>puncte</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#2836860</td>\n",
       "      <td>Luff</td>\n",
       "      <td>/problema/luff</td>\n",
       "      <td>/job_detail/2836860?action=view-source</td>\n",
       "      <td>21 ian 22 00:50:03</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#2836859</td>\n",
       "      <td>Luff</td>\n",
       "      <td>/problema/luff</td>\n",
       "      <td>/job_detail/2836859?action=view-source</td>\n",
       "      <td>21 ian 22 00:49:40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#2836858</td>\n",
       "      <td>Luff</td>\n",
       "      <td>/problema/luff</td>\n",
       "      <td>/job_detail/2836858?action=view-source</td>\n",
       "      <td>21 ian 22 00:41:56</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#2836856</td>\n",
       "      <td>Luff</td>\n",
       "      <td>/problema/luff</td>\n",
       "      <td>/job_detail/2836856?action=view-source</td>\n",
       "      <td>21 ian 22 00:36:59</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#2836855</td>\n",
       "      <td>Luff</td>\n",
       "      <td>/problema/luff</td>\n",
       "      <td>/job_detail/2836855?action=view-source</td>\n",
       "      <td>21 ian 22 00:36:26</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id problema    url_problema                               url_sursa  \\\n",
       "0  #2836860     Luff  /problema/luff  /job_detail/2836860?action=view-source   \n",
       "1  #2836859     Luff  /problema/luff  /job_detail/2836859?action=view-source   \n",
       "2  #2836858     Luff  /problema/luff  /job_detail/2836858?action=view-source   \n",
       "3  #2836856     Luff  /problema/luff  /job_detail/2836856?action=view-source   \n",
       "4  #2836855     Luff  /problema/luff  /job_detail/2836855?action=view-source   \n",
       "\n",
       "                 data  puncte  \n",
       "0  21 ian 22 00:50:03     100  \n",
       "1  21 ian 22 00:49:40       0  \n",
       "2  21 ian 22 00:41:56      70  \n",
       "3  21 ian 22 00:36:59      70  \n",
       "4  21 ian 22 00:36:26      70  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "bSUVqGCBwycM"
   },
   "outputs": [],
   "source": [
    "df_submissions.to_csv(\"submissions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JDLdzlAijhV"
   },
   "source": [
    "Exemplu scriere/citire fisier JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "SkKsP_UPiofJ"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "vec = [\n",
    "    {\"title\": \"example_1\", \"size\": 7},\n",
    "    {\"title\": \"example_2\", \"size\": 3},\n",
    "    {\"title\": \"example_3\", \"size\": 8},\n",
    "]\n",
    "\n",
    "with open(\"example.json\", \"w\") as f:\n",
    "    json.dump(vec, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1644679711408,
     "user": {
      "displayName": "Bogdan Iordache",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64",
      "userId": "11937921055418047811"
     },
     "user_tz": -120
    },
    "id": "8qpLd3OujHvP",
    "outputId": "9661a6b7-8b83-44db-c1a3-18f126be8337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'example_1', 'size': 7}, {'title': 'example_2', 'size': 3}, {'title': 'example_3', 'size': 8}]\n"
     ]
    }
   ],
   "source": [
    "with open(\"example.json\", \"r\") as f:\n",
    "    vec = json.load(f)\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L57SsXOnfd__"
   },
   "source": [
    "Un alt mod de a face scraping este sa folosim biblioteca pandas pentru a ne extrage tabele html, transformandu-le in DataFrame-uri, pe care le putem manipula foarte usor. Un exemplu util este extragerea sarbatorile legale romanesti, in anul 2022, de pe https://www.timeanddate.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: lxml in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (4.8.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 486,
     "status": "ok",
     "timestamp": 1644679255096,
     "user": {
      "displayName": "Bogdan Iordache",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64",
      "userId": "11937921055418047811"
     },
     "user_tz": -120
    },
    "id": "4KiZSYVGg1jP",
    "outputId": "b8c1093b-3c08-4272-c9f3-f84d0e15ac55"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Day</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 ian</td>\n",
       "      <td>sâmbătă</td>\n",
       "      <td>New Year's Day</td>\n",
       "      <td>National holiday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2 ian</td>\n",
       "      <td>duminică</td>\n",
       "      <td>Day after New Year's Day</td>\n",
       "      <td>National holiday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24 ian</td>\n",
       "      <td>luni</td>\n",
       "      <td>Unification Day</td>\n",
       "      <td>National holiday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19 feb</td>\n",
       "      <td>sâmbătă</td>\n",
       "      <td>Constantin Brancusi Day</td>\n",
       "      <td>Observance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24 feb</td>\n",
       "      <td>joi</td>\n",
       "      <td>Dragobete</td>\n",
       "      <td>Observance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Date       Day                      Name              Type\n",
       "0   1 ian   sâmbătă            New Year's Day  National holiday\n",
       "1   2 ian  duminică  Day after New Year's Day  National holiday\n",
       "2  24 ian      luni           Unification Day  National holiday\n",
       "3  19 feb   sâmbătă   Constantin Brancusi Day        Observance\n",
       "4  24 feb       joi                 Dragobete        Observance"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tables_df = pd.read_html('https://www.timeanddate.com/holidays/romania/2022?hol=1')\n",
    "df = tables_df[0]\n",
    "\n",
    "# Il putem curata prin a sterge liniile nule si modifica coloanele de la tuplul \"(Date, Date)\" -> \"Date\"\n",
    "df = df.dropna(axis='index')\n",
    "df.columns = ['Date', 'Day', 'Name', 'Type']\n",
    "\n",
    "# Reindexam tabelul\n",
    "df = df.reset_index(drop=\"True\")\n",
    "\n",
    "# Afisam primele 5 randuri\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1644679369336,
     "user": {
      "displayName": "Bogdan Iordache",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64",
      "userId": "11937921055418047811"
     },
     "user_tz": -120
    },
    "id": "4s6tVtnIfF1_",
    "outputId": "b6533bfb-860f-4340-cbe1-554159b629da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Day</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24 ian</td>\n",
       "      <td>luni</td>\n",
       "      <td>Unification Day</td>\n",
       "      <td>National holiday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25 apr</td>\n",
       "      <td>luni</td>\n",
       "      <td>Orthodox Easter Monday</td>\n",
       "      <td>National holiday, Orthodox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13 iun</td>\n",
       "      <td>luni</td>\n",
       "      <td>Orthodox Pentecost Monday</td>\n",
       "      <td>National holiday, Orthodox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15 aug</td>\n",
       "      <td>luni</td>\n",
       "      <td>St Mary's Day</td>\n",
       "      <td>National holiday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>31 oct</td>\n",
       "      <td>luni</td>\n",
       "      <td>Halloween</td>\n",
       "      <td>Observance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>26 dec</td>\n",
       "      <td>luni</td>\n",
       "      <td>Second day of Christmas</td>\n",
       "      <td>National holiday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Date   Day                       Name                        Type\n",
       "2   24 ian  luni            Unification Day            National holiday\n",
       "10  25 apr  luni     Orthodox Easter Monday  National holiday, Orthodox\n",
       "19  13 iun  luni  Orthodox Pentecost Monday  National holiday, Orthodox\n",
       "23  15 aug  luni              St Mary's Day            National holiday\n",
       "25  31 oct  luni                  Halloween                  Observance\n",
       "33  26 dec  luni    Second day of Christmas            National holiday"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Daca vrem se vedem sarbatorile care pica in ziua de luni\n",
    "df_luni = df.loc[df[\"Day\"] == \"luni\"]\n",
    "df_luni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1644679392984,
     "user": {
      "displayName": "Bogdan Iordache",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64",
      "userId": "11937921055418047811"
     },
     "user_tz": -120
    },
    "id": "zeX7VEyXh57m",
    "outputId": "f15cdbdc-26f9-412b-f09b-eb3d516397f1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Day</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>24 ian</td>\n",
       "      <td>luni</td>\n",
       "      <td>Unification Day</td>\n",
       "      <td>National holiday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date   Day             Name              Type\n",
       "count        6     6                6                 6\n",
       "unique       6     1                6                 3\n",
       "top     24 ian  luni  Unification Day  National holiday\n",
       "freq         1     6                1                 3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_luni.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmrzEs27iHYw"
   },
   "source": [
    "Putem salva rezultatul (la fel ca orice dictionar de python) intr-un json, ca alternativa la DataFrame - si poate fi util intr-o aplicatie pentru comunicarea cu front-end-ul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "cRwND0SBh_Ca"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "json_str = df.to_json(orient='records')\n",
    "json_result = json.loads(json_str)\n",
    "\n",
    "with open('holidays.json', 'w', encoding='utf8') as fout:\n",
    "    json.dump(json_result, fout, indent=4, sort_keys=True, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4uecs3b3MXZ"
   },
   "source": [
    "Alte biblioteci utile pentru scraping:\n",
    " * [scrapy](https://scrapy.org/) (folosit in special pentru web crawling)\n",
    " * [selenium](https://selenium-python.readthedocs.io/) (folosit pentru a simula activitatea din browser, utilizat in special in scrierea de teste pentru aplicatii front-end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFNRIfgF1cRD"
   },
   "source": [
    "## TASK: IMDb scraping (deadline: 3 martie ora 23:59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6G-0iTm14GIy"
   },
   "source": [
    "1. Pornind de la lista cu cele mai populare 250 de filme de pe IMDb ([https://www.imdb.com/chart/top/](https://www.imdb.com/chart/top/)), identificati pentru toate aceste filme link-ul catre pagina sa de recenzii.\n",
    "\n",
    "Exemplu: aici se gaseste pagina cu recenzii pentru \"The Shawshank Redemption\": [https://www.imdb.com/title/tt0111161/reviews](https://www.imdb.com/title/tt0111161/reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Kp2mSeC_9CCu"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import bs4\n",
    "\n",
    "\n",
    "def get_IMDB_page():\n",
    "    return requests.get(f\"https://www.imdb.com/chart/top/\")\n",
    "\n",
    "def parse_html(html):\n",
    "    return bs4.BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "def get_top_movies():\n",
    "    html = get_IMDB_page().content\n",
    "    soup = parse_html(html)\n",
    "\n",
    "    entries_texts = []\n",
    "    entries = soup.find_all(\"td\", class_=\"titleColumn\")\n",
    "    for entry in entries:\n",
    "        entries_texts.append(entry.find('a', href=True)['href'])\n",
    "    movie_tags = [txt[7:-1] for txt in entries_texts]\n",
    "    # print(movie_tags)\n",
    "    # print(entries_texts)\n",
    "    reviews_links = [\"https://www.imdb.com{}reviews\".format(a) for a in entries_texts]\n",
    "    return movie_tags, reviews_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wK56g-0W5I1p"
   },
   "source": [
    "2. Pentru fiecare film colectati date despre recenziile sale (titlu, text, rating, data, utlizator, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jXT34ayI5_qi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:03<00:00,  1.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_name</th>\n",
       "      <th>movie_tag</th>\n",
       "      <th>review_link_tag</th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>user_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dersu Uzala</td>\n",
       "      <td>tt0071411</td>\n",
       "      <td>rw2311238</td>\n",
       "      <td>Clearly NOT a film for the casual Kurosawa vi...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>If you haven't seen many or any of director Ak...</td>\n",
       "      <td>15 September 2010</td>\n",
       "      <td>MartinHafer</td>\n",
       "      <td>/user/ur2467618/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dersu Uzala</td>\n",
       "      <td>tt0071411</td>\n",
       "      <td>rw7542989</td>\n",
       "      <td>Languid\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Another Kurosawa film about the friendship bet...</td>\n",
       "      <td>13 November 2021</td>\n",
       "      <td>Leofwine_draca</td>\n",
       "      <td>/user/ur0482513/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dersu Uzala</td>\n",
       "      <td>tt0071411</td>\n",
       "      <td>rw6974501</td>\n",
       "      <td>surprised by Japanese legend\\n</td>\n",
       "      <td>8.0</td>\n",
       "      <td>It's 1902. Russian army explorer Captain Arsen...</td>\n",
       "      <td>28 May 2021</td>\n",
       "      <td>SnoopyStyle</td>\n",
       "      <td>/user/ur2898520/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dersu Uzala</td>\n",
       "      <td>tt0071411</td>\n",
       "      <td>rw2168879</td>\n",
       "      <td>A Man with a Beautiful Soul\\n</td>\n",
       "      <td>9.0</td>\n",
       "      <td>In 1902, a Russian army expedition is assigned...</td>\n",
       "      <td>5 December 2009</td>\n",
       "      <td>claudio_carvalho</td>\n",
       "      <td>/user/ur2488512/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dersu Uzala</td>\n",
       "      <td>tt0071411</td>\n",
       "      <td>rw7543910</td>\n",
       "      <td>\"Man is too small faced with the vastness of ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>I fully expected to find reviews of \"Dersu Uza...</td>\n",
       "      <td>13 November 2021</td>\n",
       "      <td>classicsoncall</td>\n",
       "      <td>/user/ur2707735/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    movie_name  movie_tag review_link_tag  \\\n",
       "0  Dersu Uzala  tt0071411       rw2311238   \n",
       "1  Dersu Uzala  tt0071411       rw7542989   \n",
       "2  Dersu Uzala  tt0071411       rw6974501   \n",
       "3  Dersu Uzala  tt0071411       rw2168879   \n",
       "4  Dersu Uzala  tt0071411       rw7543910   \n",
       "\n",
       "                                               title  rating  \\\n",
       "0   Clearly NOT a film for the casual Kurosawa vi...     8.0   \n",
       "1                                          Languid\\n     5.0   \n",
       "2                     surprised by Japanese legend\\n     8.0   \n",
       "3                      A Man with a Beautiful Soul\\n     9.0   \n",
       "4   \"Man is too small faced with the vastness of ...     8.0   \n",
       "\n",
       "                                                text               date  \\\n",
       "0  If you haven't seen many or any of director Ak...  15 September 2010   \n",
       "1  Another Kurosawa film about the friendship bet...   13 November 2021   \n",
       "2  It's 1902. Russian army explorer Captain Arsen...        28 May 2021   \n",
       "3  In 1902, a Russian army expedition is assigned...    5 December 2009   \n",
       "4  I fully expected to find reviews of \"Dersu Uza...   13 November 2021   \n",
       "\n",
       "               user         user_link  \n",
       "0       MartinHafer  /user/ur2467618/  \n",
       "1    Leofwine_draca  /user/ur0482513/  \n",
       "2       SnoopyStyle  /user/ur2898520/  \n",
       "3  claudio_carvalho  /user/ur2488512/  \n",
       "4    classicsoncall  /user/ur2707735/  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pdb\n",
    "\n",
    "\n",
    "def get_reviews_page(movie_tag, paginationKey=''):\n",
    "    link = ''\n",
    "    if paginationKey != '':\n",
    "        link = \"https://www.imdb.com/title/{}/reviews/_ajax?paginationKey={}\".format(movie_tag, paginationKey)\n",
    "    else:\n",
    "        link = \"https://www.imdb.com/title/{}/reviews\".format(movie_tag)\n",
    "    return requests.get(link)\n",
    "\n",
    "\n",
    "def scrape_reviews(movie):\n",
    "    # determinam numarul total de submisii\n",
    "    link = get_reviews_page(movie)\n",
    "    soup = parse_html(link.content)\n",
    "    movie_name = soup.find(\"div\", class_=\"parent\").find(\"a\").text\n",
    "    paginationKey = ''\n",
    "    review_count_text = soup.find(\"div\", class_=\"header\").find(\"span\").text\n",
    "    review_count = int(\"\".join(review_count_text.split(\" \")[0].split(\",\")))\n",
    "#     print(submission_count)\n",
    "    # vom salva in acest dictionar datele despre submisiile extrase, structura aceasta\n",
    "    # ne va ajuta ulterior sa construim un tabel (dataframe) folosind pandas\n",
    "    reviews_dict = {\n",
    "        \"movie_name\":[movie_name]*(review_count),\n",
    "        \"movie_tag\": [movie]*(review_count),\n",
    "        \"review_link_tag\": [],\n",
    "        \"title\": [],\n",
    "        \"rating\": [],\n",
    "        \"text\": [],\n",
    "        \"date\": [],\n",
    "        \"user\": [],\n",
    "        \"user_link\": []\n",
    "    }\n",
    "    \n",
    "#     review_pages = int(review_count / 25)\n",
    "#     if review_count % 25 != 0:\n",
    "#         review_pages += 1\n",
    "    \n",
    "    # accesam pagini cu submisii in grupuri de 25\n",
    "    for review_page in tqdm(range(0, review_count, 25)):\n",
    "        if paginationKey != '':\n",
    "            link = get_reviews_page(movie, paginationKey)\n",
    "            soup = parse_html(link.content)\n",
    "        \n",
    "        reviews = soup.find_all(\"div\", class_=\"lister-item\")\n",
    "\n",
    "        for review in reviews:\n",
    "            try:\n",
    "                link_tag = review[\"data-review-id\"]\n",
    "            except Exception:\n",
    "                link_tag = None\n",
    "\n",
    "            reviews_dict[\"review_link_tag\"].append(link_tag)\n",
    "            \n",
    "            try:\n",
    "                title = review.find(\"a\", class_=\"title\").text\n",
    "            except Exception:\n",
    "                title = None\n",
    "            reviews_dict[\"title\"].append(title)\n",
    "            \n",
    "            try:\n",
    "                rating = int(review.find(\"span\", class_=\"rating-other-user-rating\").find(\"span\").text)\n",
    "            except Exception:\n",
    "                rating = None\n",
    "            reviews_dict[\"rating\"].append(rating)\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                date = review.find(\"span\", class_=\"review-date\").text\n",
    "            except Exception:\n",
    "                date = None\n",
    "            reviews_dict[\"date\"].append(date)\n",
    "            \n",
    "            try:\n",
    "                text = review.find(\"div\", class_=\"text show-more__control\").text\n",
    "            except Exception:\n",
    "                text = None\n",
    "            reviews_dict[\"text\"].append(text)\n",
    "            \n",
    "            try:\n",
    "                user_soup = review.find(\"span\", class_=\"display-name-link\").find(\"a\")\n",
    "                user = user_soup.text\n",
    "                user_link = user_soup[\"href\"]\n",
    "            except Exception:\n",
    "                user = None\n",
    "                user_link = None\n",
    "            reviews_dict[\"user\"].append(user)\n",
    "            reviews_dict[\"user_link\"].append(user_link)     \n",
    "            \n",
    "        try:\n",
    "            paginationKey = soup.find(\"div\", class_=\"load-more-data\")[\"data-key\"]\n",
    "#             print(paginationKey)\n",
    "        except Exception:\n",
    "            break\n",
    "#     print(len(reviews_dict[\"movie_name\"]),len(reviews_dict[\"user\"]), \" \", review_count)\n",
    "    return pd.DataFrame(reviews_dict)\n",
    "\n",
    "# test = scrape_reviews(\"tt0071411\")\n",
    "# test.head()\n",
    "# for review_link in review_links:\n",
    "#     reviews = []\n",
    "#     for review in review_link:\n",
    "#         review.scrape_review()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhcG_zFK6AFE"
   },
   "source": [
    "3. Creati un dataset de recenzii, pentru fiecare recenzie stocati:\n",
    " * filmul caruia ii apartine\n",
    " * titlul recenziei\n",
    " * textul recenziei\n",
    " * ratingul\n",
    " * data\n",
    " * utilizator\n",
    "\n",
    " Salvati datasetul intr-un fisier JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "4BYOWe4C6wz3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:03<00:00,  1.16it/s]\n",
      " 86%|████████████████████████████████████████████████████████████████████████            | 6/7 [00:06<00:01,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import json\n",
    "\n",
    "def build_Json_dataset():\n",
    "    movie_tags, reviews_links = get_top_movies()\n",
    "    data_set = pd.DataFrame({\n",
    "        \"movie_name\":[],\n",
    "        \"movie_tag\": [],\n",
    "        \"review_link_tag\": [],\n",
    "        \"title\": [],\n",
    "        \"rating\": [],\n",
    "        \"text\": [],\n",
    "        \"date\": [],\n",
    "        \"user\": [],\n",
    "        \"user_link\": []\n",
    "    })\n",
    "#     movie_tags = [\"tt0071411\",\"tt0048473\"]\n",
    "    for movie in movie_tags:\n",
    "        data_set = pd.concat([data_set, scrape_reviews(movie)])\n",
    "#     display(data_set.tail())\n",
    "    data_set = data_set.reset_index(drop=True)\n",
    "    data_needed = data_set[[\"movie_name\", \"title\", \"text\", \"rating\", \"date\", \"user\"]]\n",
    "#     display(data_needed.tail())\n",
    "    with open(\"example.json\", \"w\") as f:\n",
    "        json.dump(data_needed.to_json(), f)\n",
    "\n",
    "\n",
    "    \n",
    "build_Json_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtSpO-Ss6yF_"
   },
   "source": [
    "4. Pe o pagina cu recenzii putem gasi un numar mic de astfel de date. Butonul de \"Load more\" de la final, cand este apasat, produce un request care returneaza HTML-ul urmatoarelor recenzii. Folosind aceasta logica colectati automat pentru fiecare film un numar mai mare de recenzii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aO3K690x7m4I"
   },
   "outputs": [],
   "source": [
    "# deja implementat la Task 2."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab1_draft",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
